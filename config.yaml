explanation_text:
  RF: Random forests or random decision forests is an ensemble learning method for
    classification, regression and other tasks that operates by constructing a multitude
    of decision trees at training time. For classification tasks, the output of the
    random forest is the class selected by most trees. For regression tasks, the mean
    or average prediction of the individual trees is returned.[1][2] Random decision
    forests correct for decision trees' habit of overfitting to their training set.
  RF_max_depth: The maximum depth of the tree
  RF_min_samples_split: "The minimum number of samples required to split an internal\
    \ node:\n - If int, then consider min_samples_split as the minimum number.\n -\
    \ If float, then min_samples_split is a fraction and ceil(min_samples_split *\
    \ n_samples) are the minimum number of samples for each split."
  RF_n_estimator: The number of trees in the forest.
  RF_min_samples_leaf : "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression."
  RF_bootstrap : Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.
  RF_max_samples : If bootstrap is True, the number of samples to draw from X to train each base estimator.
  SVR_C: Regularization parameter. The strength of the regularization is inversely
    proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.
  SVR_degree: "Degree of the polynomial kernel function (\xE2\u20AC\u02DCpoly\xE2\u20AC\
    \u2122). Ignored by all other kernels."
  SVR_epsilon: Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within
    which no penalty is associated in the training loss function with points predicted
    within a distance epsilon from the actual value.
  SVR_kernel: A kernel is a function used in SVM for helping to solve problems. They
    provide shortcuts to avoid complex calculations. The amazing thing about kernel
    is that we can go to higher dimensions and perform smooth calculations with the
    help of it. We can go up to an infinite number of dimensions using kernels. Sometimes,
    we cannot have a hyperplane for certain problems. This problem arises when we
    go up to higher dimensions and try to form a hyperplane. A kernel helps to form
    the hyperplane in the higher dimension without raising the complexity.
  SVR_coef0: Independent term in kernel function. It is only significant in poly and sigmoid.
  SVR_gamma: "Kernel coefficient for rbf, poly and sigmoid.\n
              - if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,\n
              - if auto, uses 1 / n_features."
  SVR_paragraph_1: In machine learning, Support Vector Machines are supervised learning
    models with associated learning algorithms that analyze data used for classification
    and regression analysis. In Support Vector Regression, the straight line that
    is required to fit the data is referred to as hyperplane.
  SVR_paragraph_2: The objective of a support vector machine algorithm is to find
    a hyperplane in an n-dimensional space that distinctly classifies the data points.
    The data points on either side of the hyperplane that are closest to the hyperplane
    are called Support Vectors. These influence the position and orientation of the
    hyperplane and thus help build the SVM.
  arima_d: Number of lagged forecast errors that should go into the Model
  arima_p: The minimum number of differencing needed to make the series stationary.
    And if the time series is already stationary, then d = 0
  arima_q: Number of lags of Y to be used as predictors
  compare_all_model: " This method will compare all the statistic, machine learning\
    \ and deep learning model. The model that will be compared is : \n  Statistic\
    \ : The statistic model will only work when the data is univariable with 1 timelag\
    \  \n  - ARIMA \n - SARIMAX \n  Machine learning : \n  - Support Vector Regression\
    \ \n - Random Forest \n  \n Deep Learning : \n - LSTM Model : with 2 LSTM hidden\
    \ layer, the first layer will have 8 hidden unit while the second layer have 4\
    \ hidden unit \n - Dense Model : with 2 dense hidden layer, the first layer will\
    \ have 8 hidden unit while the second layer have 4 hidden unit"
  home: "Tugas Akhir Teknik Telekomunikasi \n - Danendra Athallariq Harya Putra 18118006\
    \ \n - Rifqi Syahri Ramadhani 18118008"
  sarimax_D: ''
  sarimax_P: ''
  sarimax_Q: ''
  sarimax_s: ''
  tf_early_stop: optimization technique used to reduce overfitting without compromising
    on model accuracy. The main idea behind early stopping is to stop training before
    a model starts to overfit.
  tf_loss: Method of evaluating how well specific algorithm models the given data.
    If predictions deviates too much from actual results, loss function would cough
    up a very large number. Gradually, with the help of some optimization function,
    loss function learns to reduce the error in prediction.
  tf_lr: ' Tuning parameter in an optimization algorithm that determines the step
    size at each iteration while moving toward a minimum of a loss function.[1] Since
    it influences to what extent newly acquired information overrides old information,
    it metaphorically represents the speed at which a machine learning model learns'
  tf_metric: Metrics to be evaluated by the model during training and testing
  tf_optimizer: 'An optimizer is a function or an algorithm that modifies the attributes
    of the neural network, such as weights and learning rate. Thus, it helps in reducing
    the overall loss and improve the accuracy. '
  tf_patience: The model will see the last n epochs, if at the last n epoch the model
    don't improve then early stopping will activate
<<<<<<< HEAD
  FNN : "A feedforward neural network (FNN) is an artificial neural network wherein connections between the nodes do not form a cycle. As such, it is different from its descendant: recurrent neural networks.The feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction forward from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network."
  LSTM : "Long short-term memory (LSTM) is an artificial neural network used in the fields of artificial intelligence and deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. Such a recurrent neural network can process not only single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition, machine translation, robot control, video games, and healthcare. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell."
=======
>>>>>>> 615aaf98d654dd6c6234cffd52e3996b2f34fdea
model_tf:
  layers: null
  units: null

explanation_text:
  FNN: 'A feedforward neural network (FNN) is an artificial neural network wherein
    connections between the nodes do not form a cycle. As such, it is different from
    its descendant: recurrent neural networks.The feedforward neural network was the
    first and simplest type of artificial neural network devised. In this network,
    the information moves in only one direction forward from the input nodes, through
    the hidden nodes (if any) and to the output nodes. There are no cycles or loops
    in the network.'
  LSTM: Long short-term memory (LSTM) is an artificial neural network used in the
    fields of artificial intelligence and deep learning. Unlike standard feedforward
    neural networks, LSTM has feedback connections. Such a recurrent neural network
    can process not only single data points (such as images), but also entire sequences
    of data (such as speech or video). For example, LSTM is applicable to tasks such
    as unsegmented, connected handwriting recognition, speech recognition, machine
    translation, robot control, video games, and healthcare. A common LSTM unit is
    composed of a cell, an input gate, an output gate and a forget gate. The cell
    remembers values over arbitrary time intervals and the three gates regulate the
    flow of information into and out of the cell.
  RF: Random forests or random decision forests is an ensemble learning method for
    classification, regression and other tasks that operates by constructing a multitude
    of decision trees at training time. For classification tasks, the output of the
    random forest is the class selected by most trees. For regression tasks, the mean
    or average prediction of the individual trees is returned.[1][2] Random decision
    forests correct for decision trees' habit of overfitting to their training set.
  RF_bootstrap: Whether bootstrap samples are used when building trees. If False,
    the whole dataset is used to build each tree.
  RF_max_depth: The maximum depth of the tree
  RF_max_samples: If bootstrap is True, the number of samples to draw from X to train
    each base estimator.
  RF_min_samples_leaf: The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at least min_samples_leaf
    training samples in each of the left and right branches. This may have the effect
    of smoothing the model, especially in regression.
  RF_min_samples_split: "The minimum number of samples required to split an internal\
    \ node:\n - If int, then consider min_samples_split as the minimum number.\n -\
    \ If float, then min_samples_split is a fraction and ceil(min_samples_split *\
    \ n_samples) are the minimum number of samples for each split."
  RF_n_estimator: The number of trees in the forest.
  SVR_C: Regularization parameter. The strength of the regularization is inversely
    proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.
  SVR_coef0: Independent term in kernel function. It is only significant in poly and
    sigmoid.
  SVR_degree: "Degree of the polynomial kernel function (\xE2\u20AC\u02DCpoly\xE2\u20AC\
    \u2122). Ignored by all other kernels."
  SVR_epsilon: Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within
    which no penalty is associated in the training loss function with points predicted
    within a distance epsilon from the actual value.
  SVR_gamma: "Kernel coefficient for rbf, poly and sigmoid.\n - if gamma='scale' (default)\
    \ is passed then it uses 1 / (n_features * X.var()) as value of gamma,\n - if\
    \ auto, uses 1 / n_features."
  SVR_kernel: A kernel is a function used in SVM for helping to solve problems. They
    provide shortcuts to avoid complex calculations. The amazing thing about kernel
    is that we can go to higher dimensions and perform smooth calculations with the
    help of it. We can go up to an infinite number of dimensions using kernels. Sometimes,
    we cannot have a hyperplane for certain problems. This problem arises when we
    go up to higher dimensions and try to form a hyperplane. A kernel helps to form
    the hyperplane in the higher dimension without raising the complexity.
  SVR_paragraph_1: In machine learning, Support Vector Machines are supervised learning
    models with associated learning algorithms that analyze data used for classification
    and regression analysis. In Support Vector Regression, the straight line that
    is required to fit the data is referred to as hyperplane.
  SVR_paragraph_2: The objective of a support vector machine algorithm is to find
    a hyperplane in an n-dimensional space that distinctly classifies the data points.
    The data points on either side of the hyperplane that are closest to the hyperplane
    are called Support Vectors. These influence the position and orientation of the
    hyperplane and thus help build the SVM.
  arima_d: Number of lagged forecast errors that should go into the Model
  arima_p: The minimum number of differencing needed to make the series stationary.
    And if the time series is already stationary, then d = 0
  arima_q: Number of lags of Y to be used as predictors
  compare_all_model: " This method will compare all the statistic, machine learning\
    \ and deep learning model. The model that will be compared is : \n  Statistic\
    \ : The statistic model will only work when the data is univariable with 1 timelag\
    \  \n  - ARIMA \n - SARIMAX \n  Machine learning : \n  - Support Vector Regression\
    \ \n - Random Forest \n  \n Deep Learning : \n - LSTM Model : with 2 LSTM hidden\
    \ layer, the first layer will have 8 hidden unit while the second layer have 4\
    \ hidden unit \n - Dense Model : with 2 dense hidden layer, the first layer will\
    \ have 8 hidden unit while the second layer have 4 hidden unit"
  home: "Tugas Akhir Teknik Telekomunikasi \n - Danendra Athallariq Harya Putra 18118006\
    \ \n - Rifqi Syahri Ramadhani 18118008"
  sarimax_D: ''
  sarimax_P: ''
  sarimax_Q: ''
  sarimax_s: ''
  tf_early_stop: optimization technique used to reduce overfitting without compromising
    on model accuracy. The main idea behind early stopping is to stop training before
    a model starts to overfit.
  tf_loss: Method of evaluating how well specific algorithm models the given data.
    If predictions deviates too much from actual results, loss function would cough
    up a very large number. Gradually, with the help of some optimization function,
    loss function learns to reduce the error in prediction.
  tf_lr: ' Tuning parameter in an optimization algorithm that determines the step
    size at each iteration while moving toward a minimum of a loss function.[1] Since
    it influences to what extent newly acquired information overrides old information,
    it metaphorically represents the speed at which a machine learning model learns'
  tf_metric: Metrics to be evaluated by the model during training and testing
  tf_optimizer: 'An optimizer is a function or an algorithm that modifies the attributes
    of the neural network, such as weights and learning rate. Thus, it helps in reducing
    the overall loss and improve the accuracy. '
  tf_patience: The model will see the last n epochs, if at the last n epoch the model
    don't improve then early stopping will activate
model_tf:
  layers: null
  units: null
